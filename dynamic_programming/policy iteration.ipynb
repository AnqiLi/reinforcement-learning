{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) Initialization\n",
    "V = np.random.uniform(A*B)\n",
    "policy(S) = random\n",
    "\n",
    "2) Policy evaluation\n",
    "theta = 0.001\n",
    "while True:\n",
    "    delta = 0\n",
    "    for state in states:\n",
    "        v = V[state]\n",
    "        acum = 0\n",
    "        for new_state in states:\n",
    "            acum +=  prob(state, policy(state), new_state) * (reward + gamma * V[new_state])\n",
    "        \n",
    "        delta = max(delta, abs(v - V[state]))\n",
    "    \n",
    "    if deleta < theta: break\n",
    "\n",
    "3) Policy improvement\n",
    "policy_stable = true\n",
    "for state in states:\n",
    "    old_action = policy(state)\n",
    "    \n",
    "    for new_state in states:\n",
    "        action_state_values = prob(state, policy(state), new_state) * (reward + gamma * V[new_state])\n",
    "        max_action = argmax_Action(action_state_values)\n",
    "        policy(state) = max_action\n",
    "        \n",
    "    if old_action != policy(state): policy_stable = false & goto 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld(object):\n",
    "    def __init__(self):\n",
    "        self.states = range(16)\n",
    "        self.num_states = len(self.states)\n",
    "        self.actions = [\"up\", \"down\", \"right\", \"left\"]\n",
    "        \n",
    "        #state_policy = [1, 0, 0, 0]\n",
    "        #self.policy = np.zeros([self.num_states, len(self.actions)])\n",
    "        #for p_index in range(self.policy.shape[0]):\n",
    "        #    self.policy[p_index] = np.random.permutation(state_policy)\n",
    "            \n",
    "        #self.policy = self.policy.reshape(self.num_states, len(self.actions), 1)\n",
    "            \n",
    "        self.reward = np.full([self.num_states, len(self.actions), self.num_states], -1)\n",
    "        self.probabilities = np.full([self.num_states, len(self.actions), self.num_states], 0)\n",
    "        \n",
    "        for from_state in self.states:\n",
    "            for action in self.actions:\n",
    "                action_index = self.actions.index(action)\n",
    "                for to_state in self.states:\n",
    "                    p = self.probability(from_state, action, to_state)\n",
    "                    self.probabilities[from_state][action_index][to_state] = p\n",
    "                    \n",
    "    \n",
    "    def probability(self, from_position, action, to_position):\n",
    "        num_rows, num_cols = self.num_states / 4, self.num_states / 4\n",
    "        from_x, from_y = from_position//num_rows, from_position%num_cols\n",
    "        to_x, to_y = to_position//num_rows, to_position%num_cols\n",
    "        \n",
    "        if abs(to_x - from_x) > 1: return 0\n",
    "        if abs(to_y - from_y) > 1: return 0\n",
    "        \n",
    "        if from_x == 0 and from_x == to_x and from_y == to_y and action == \"up\":\n",
    "            return 1\n",
    "        if from_x == num_rows-1 and from_x == to_x and from_y == to_y and action == \"down\":\n",
    "            return 1\n",
    "        if from_y == 0 and from_x == to_x and from_y == to_y and action == \"left\":\n",
    "            return 1\n",
    "        if from_y == num_cols-1 and from_x == to_x and from_y == to_y and action == \"right\":\n",
    "            return 1\n",
    "        \n",
    "        if from_x == to_x and from_y == to_y: return 0\n",
    "        \n",
    "        if from_x == to_x + 1 and from_y == to_y + 1: return 0\n",
    "        if from_x == to_x - 1 and from_y == to_y - 1: return 0\n",
    "        if from_x == to_x + 1 and from_y == to_y - 1: return 0\n",
    "        if from_x == to_x - 1 and from_y == to_y + 1: return 0\n",
    "        \n",
    "        if from_x + 1 == to_x and action != \"down\": return 0\n",
    "        if from_x - 1 == to_x and action != \"up\": return 0\n",
    "        if from_y + 1 == to_y and action != \"right\": return 0\n",
    "        if from_y - 1 == to_y and action != \"left\": return 0\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "    def policy_iteration(self):\n",
    "        pass\n",
    "\n",
    "def policy_eval(policy, value_state, grid, discount_factor=1.0, theta=0.00001):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        old_value_state = value_state\n",
    "        \n",
    "        new_v_action_state = (value_state*discount_factor + grid.reward)*grid.probabilities*policy\n",
    "        new_v = new_v_action_state.sum(axis=2).sum(axis=1)\n",
    "        new_v.put([0, -1], [0, 0])\n",
    "        value_state = new_v\n",
    "        \n",
    "        delta = max(delta, (old_value_state - value_state).max())\n",
    "        if delta < theta:\n",
    "            print(i)\n",
    "            break\n",
    "\n",
    "    return value_state        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "try again\n",
      "1\n",
      "try again\n",
      "2\n",
      "try again\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 3, 3, 1],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 0, 1, 1],\n",
       "        [0, 2, 2, 1]]), array([[ 0., -1., -2., -3.],\n",
       "        [-1., -2., -3., -2.],\n",
       "        [-2., -3., -2., -1.],\n",
       "        [-3., -2., -1.,  0.]]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = Gridworld()\n",
    "\n",
    "discount_factor = 1\n",
    "theta = 0.0001\n",
    "value_state = np.full(grid.num_states, 0)\n",
    "policy = np.full([grid.num_states, len(grid.actions), 1], 1/len(grid.actions))\n",
    "\n",
    "def policy_iteration(policy, value_state, grid):\n",
    "    value_state = policy_eval(policy, value_state, grid, discount_factor=discount_factor, theta=theta)\n",
    "    old_policy = policy\n",
    "    \n",
    "    new_policy = (grid.probabilities * (grid.reward + discount_factor*value_state)).sum(axis=2).argmax(axis=1)\n",
    "    new_policy = new_policy.reshape(new_policy.shape[0], 1)\n",
    "    return new_policy, value_state\n",
    "\n",
    "for i in range(20):\n",
    "    new_policy_argmax, value_state = policy_iteration(policy, value_state, grid)\n",
    "    new_policy = np.zeros([grid.num_states, len(grid.actions)])\n",
    "    new_policy[range(len(policy)), new_policy_argmax.reshape(new_policy_argmax.shape[0],)] = 1\n",
    "    new_policy = new_policy.reshape([grid.num_states, len(grid.actions), 1])\n",
    "\n",
    "    diff = np.abs(policy - new_policy).max(axis=1)\n",
    "    policy = new_policy\n",
    "    if diff[diff != 0].shape[0] != 0:\n",
    "        print(\"try again\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "policy.reshape(16,4).argmax(axis=1).reshape(4, 4), value_state.reshape(4,4)\n",
    "        \n",
    "\n",
    "#np.asmatrix(p.reshape(16,4))[:,[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
