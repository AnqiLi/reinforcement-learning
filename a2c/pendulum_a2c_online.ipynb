{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, input): return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert numpy arrays to tensors\n",
    "def t(x):\n",
    "    x = np.array(x) if not isinstance(x, np.ndarray) else x\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor module, categorical actions only\n",
    "import math\n",
    "leaky = torch.nn.LeakyReLU()\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 32),\n",
    "            # nn.BatchNorm1d(32),\n",
    "            Mish(),\n",
    "        )\n",
    "        self.means_head = nn.Sequential(\n",
    "            nn.Linear(32, n_actions),\n",
    "            # nn.BatchNorm1d(n_actions),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.stds_head = nn.Sequential(\n",
    "            nn.Linear(32, n_actions),\n",
    "            #Â nn.BatchNorm1d(n_actions),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        data = self.model(X)\n",
    "        means = self.means_head(data)\n",
    "        stds = torch.clamp(self.stds_head(data), 1e-3, 2)\n",
    "        \n",
    "        dists = torch.distributions.Normal(means*2, stds)\n",
    "        \n",
    "        return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Critic module\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 32),\n",
    "            # nn.BatchNorm1d(n_actions),\n",
    "            Mish(),\n",
    "            nn.Linear(32, 32),\n",
    "            # nn.BatchNorm1d(n_actions),\n",
    "            Mish(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "actor = Actor(state_dim, n_actions)\n",
    "critic = Critic(state_dim)\n",
    "adam_actor = torch.optim.Adam(actor.parameters(), lr=5e-3)#, weight_decay=0.001)\n",
    "adam_critic = torch.optim.Adam(critic.parameters(), lr=5e-3)#, weight_decay=0.001)\n",
    "gamma = 0.98\n",
    "entropy_beta = 0.01\n",
    "memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(rewards, dones, gamma):\n",
    "    ret = 0\n",
    "    discounted = []\n",
    "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "        ret = reward + ret * gamma * (1-done)\n",
    "        discounted.append(ret)\n",
    "    \n",
    "    return discounted[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.48505, 0.995, 0.5, 0.0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted_rewards([0.5, 0.5, 0.5] + [0], [0, 0, 0, 0], 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(memory):\n",
    "    actions = []\n",
    "    states = []\n",
    "    next_states = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    for action, reward, state, next_state, done in memory:\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        next_states.append(next_state)\n",
    "        dones.append(done)\n",
    "    \n",
    "    if dones[-1] == 0:\n",
    "        next_value = critic(t([next_states[-1]])).data.numpy()[0][0]\n",
    "        # import pdb; pdb.set_trace()\n",
    "        rewards = discounted_rewards(rewards + [next_value], dones + [0], gamma)[:-1]\n",
    "    else:\n",
    "        rewards = discounted_rewards(rewards, dones, gamma)\n",
    "    \n",
    "    actions = t(actions).view(-1, 1)\n",
    "    states = t(states)\n",
    "    next_states = t(next_states)\n",
    "    rewards = t(rewards).view(-1, 1)\n",
    "    dones = t(dones).view(-1, 1)\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    td_target = rewards # + gamma*critic(next_states)*(1-dones)\n",
    "    value = critic(states)\n",
    "    advantage = td_target - value\n",
    "    \n",
    "    norm_dists = actor(states)\n",
    "    logs_probs = norm_dists.log_prob(actions)\n",
    "    entropy = norm_dists.entropy()\n",
    "\n",
    "    actor_loss = (-logs_probs*advantage.detach()).mean() - entropy.mean()\n",
    "    \n",
    "    writer.add_scalar(\"losses/entropy\", entropy.mean())    \n",
    "    writer.add_scalar(\"losses/actor\", actor_loss)\n",
    "    adam_actor.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    adam_actor.step()\n",
    "    \n",
    "    critic_loss = F.mse_loss(td_target, value)\n",
    "    writer.add_scalar(\"losses/critic\", critic_loss)\n",
    "    adam_critic.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    adam_critic.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 , episode reward: -1126.9103188894364\n",
      "episode: 10 , episode reward: -1452.051985593481\n",
      "episode: 20 , episode reward: -1613.5040760671034\n",
      "episode: 30 , episode reward: -1329.1549960861225\n",
      "episode: 40 , episode reward: -1618.15319021979\n",
      "episode: 50 , episode reward: -1631.396920031989\n",
      "episode: 60 , episode reward: -1420.4970669995278\n",
      "episode: 70 , episode reward: -1539.0922515409884\n",
      "episode: 80 , episode reward: -1449.5661809410162\n",
      "episode: 90 , episode reward: -1331.4153942374749\n",
      "episode: 100 , episode reward: -1398.200739801941\n",
      "episode: 110 , episode reward: -1067.3703625661644\n",
      "episode: 120 , episode reward: -1727.9722994786812\n",
      "episode: 130 , episode reward: -1416.5151800883557\n",
      "episode: 140 , episode reward: -1839.8758935481235\n",
      "episode: 150 , episode reward: -1721.1004545989472\n",
      "episode: 160 , episode reward: -1827.7191086447374\n",
      "episode: 170 , episode reward: -1823.763778450008\n",
      "episode: 180 , episode reward: -1778.0354832163289\n",
      "episode: 190 , episode reward: -1728.17729115796\n",
      "episode: 200 , episode reward: -1798.8786294033175\n",
      "episode: 210 , episode reward: -1567.04910665886\n",
      "episode: 220 , episode reward: -1737.3475572036175\n",
      "episode: 230 , episode reward: -1697.1596137369818\n",
      "episode: 240 , episode reward: -1597.3751000435147\n",
      "episode: 250 , episode reward: -1602.0317263397542\n",
      "episode: 260 , episode reward: -1756.9372466941736\n",
      "episode: 270 , episode reward: -1576.7754890368144\n",
      "episode: 280 , episode reward: -1568.602280975931\n",
      "episode: 290 , episode reward: -1722.7644869263638\n",
      "episode: 300 , episode reward: -1607.820651462574\n",
      "episode: 310 , episode reward: -1537.8077542372214\n",
      "episode: 320 , episode reward: -1742.1113324492571\n",
      "episode: 330 , episode reward: -1792.2228908222664\n",
      "episode: 340 , episode reward: -1473.6132057658965\n",
      "episode: 350 , episode reward: -1710.5755166686893\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "total_steps = 0\n",
    "memory = []\n",
    "\n",
    "for i in range(5000):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    max_steps = 5\n",
    "    steps = 0\n",
    "    memory.clear()\n",
    "\n",
    "    while not done:\n",
    "        dists = actor(t(state))\n",
    "        actions = dists.sample()\n",
    "        actions_clamped = torch.clamp(actions, env.action_space.low.min(), env.action_space.high.max())\n",
    "        \n",
    "        next_state, reward, done, info = env.step(actions_clamped.detach().data.numpy())\n",
    "        \n",
    "        memory.append((actions, reward, state, next_state, done))\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        total_steps += 1\n",
    "        \n",
    "        writer.add_scalar(\"dists/mean\", dists.loc[0], global_step=total_steps)\n",
    "        writer.add_scalar(\"dists/scale\", dists.scale[0], global_step=total_steps)\n",
    "\n",
    "        if done or (steps % max_steps == 0):\n",
    "            train(memory)\n",
    "            memory.clear()\n",
    "            \n",
    "        # env.render()\n",
    "    \n",
    "    if len(episode_rewards) % 10 == 0:\n",
    "        print(\"episode:\", len(episode_rewards), \", episode reward:\", total_reward)\n",
    "    writer.add_scalar(\"episode_reward\", total_reward, global_step=total_steps)\n",
    "    episode_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(episode_rewards)), episode_rewards, s=2)\n",
    "plt.title(\"Total reward per episode (online)\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.4888], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2877,  0.0264, -0.1438,  0.5027,  1.0066, -0.1544,  0.4802, -0.1561,\n",
       "         0.2354, -0.0054,  0.3285,  0.1031,  0.7267, -0.0575, -0.0306,  0.7434,\n",
       "         0.4232, -0.1967,  0.2230, -0.1707,  0.0087, -0.2611,  0.7855,  0.3474,\n",
       "         0.8359,  0.1104, -0.2812, -0.2202,  0.3276,  0.1480, -0.3086, -0.1037],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = actor.model(t(state))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0014], grad_fn=<SoftplusBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.stds_head(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
