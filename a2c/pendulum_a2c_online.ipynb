{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, input): return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert numpy arrays to tensors\n",
    "def t(x):\n",
    "    x = np.array(x) if not isinstance(x, np.ndarray) else x\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "        \n",
    "        logstds_param = nn.Parameter(torch.full((n_actions,), 0.1))\n",
    "        self.register_parameter(\"logstds\", logstds_param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        means = self.model(X)\n",
    "        stds = torch.clamp(self.logstds.exp(), 1e-3, 50)\n",
    "        \n",
    "        return torch.distributions.Normal(means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Critic module\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(rewards, dones, gamma):\n",
    "    ret = 0\n",
    "    discounted = []\n",
    "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "        ret = reward + ret * gamma * (1-done)\n",
    "        discounted.append(ret)\n",
    "    \n",
    "    return discounted[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_memory(memory, gamma=0.99, discount_rewards=True):\n",
    "    actions = []\n",
    "    states = []\n",
    "    next_states = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "\n",
    "    for action, reward, state, next_state, done in memory:\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        next_states.append(next_state)\n",
    "        dones.append(done)\n",
    "    \n",
    "    if discount_rewards:\n",
    "        if False and dones[-1] == 0:\n",
    "            rewards = discounted_rewards(rewards + [last_value], dones + [0], gamma)[:-1]\n",
    "        else:\n",
    "            rewards = discounted_rewards(rewards, dones, gamma)\n",
    "\n",
    "    actions = t(actions).view(-1, 1)\n",
    "    states = t(states)\n",
    "    next_states = t(next_states)\n",
    "    rewards = t(rewards).view(-1, 1)\n",
    "    dones = t(dones).view(-1, 1)\n",
    "    return actions, rewards, states, next_states, dones\n",
    "\n",
    "def clip_grad_norm_(module, max_grad_norm):\n",
    "    nn.utils.clip_grad_norm_([p for g in module.param_groups for p in g[\"params\"]], max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CLearner():\n",
    "    def __init__(self, actor, critic, gamma=0.9, entropy_beta=0,\n",
    "                 actor_lr=4e-4, critic_lr=4e-3, max_grad_norm=0.5):\n",
    "        self.gamma = gamma\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.actor_optim = torch.optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optim = torch.optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def learn(self, memory, steps, discount_rewards=True):\n",
    "        actions, rewards, states, next_states, dones = process_memory(memory, self.gamma, discount_rewards)\n",
    "\n",
    "        if discount_rewards:\n",
    "            td_target = rewards\n",
    "        else:\n",
    "            td_target = rewards + self.gamma*critic(next_states)*(1-dones)\n",
    "        value = critic(states)\n",
    "        advantage = td_target - value\n",
    "\n",
    "        # actor\n",
    "        norm_dists = self.actor(states)\n",
    "        logs_probs = norm_dists.log_prob(actions)\n",
    "        entropy = norm_dists.entropy().mean()\n",
    "        \n",
    "        actor_loss = (-logs_probs*advantage.detach()).mean() - entropy*self.entropy_beta\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        \n",
    "        clip_grad_norm_(self.actor_optim, self.max_grad_norm)\n",
    "        writer.add_histogram(\"gradients/actor\",\n",
    "                             torch.cat([p.grad.view(-1) for p in self.actor.parameters()]), global_step=steps)\n",
    "        writer.add_histogram(\"parameters/actor\",\n",
    "                             torch.cat([p.data.view(-1) for p in self.actor.parameters()]), global_step=steps)\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # critic\n",
    "        critic_loss = F.mse_loss(td_target, value)\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        clip_grad_norm_(self.critic_optim, self.max_grad_norm)\n",
    "        writer.add_histogram(\"gradients/critic\",\n",
    "                             torch.cat([p.grad.view(-1) for p in self.critic.parameters()]), global_step=steps)\n",
    "        writer.add_histogram(\"parameters/critic\",\n",
    "                             torch.cat([p.data.view(-1) for p in self.critic.parameters()]), global_step=steps)\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # reports\n",
    "        writer.add_scalar(\"losses/log_probs\", -logs_probs.mean(), global_step=steps)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy, global_step=steps) \n",
    "        writer.add_scalar(\"losses/entropy_beta\", self.entropy_beta, global_step=steps) \n",
    "        writer.add_scalar(\"losses/actor\", actor_loss, global_step=steps)\n",
    "        writer.add_scalar(\"losses/advantage\", advantage.mean(), global_step=steps)\n",
    "        writer.add_scalar(\"losses/critic\", critic_loss, global_step=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state = None\n",
    "        self.done = True\n",
    "        self.steps = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def reset(self):\n",
    "        self.episode_reward = 0\n",
    "        self.done = False\n",
    "        self.state = self.env.reset()\n",
    "    \n",
    "    def run(self, max_steps, memory=None):\n",
    "        if not memory: memory = []\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            if self.done: self.reset()\n",
    "            \n",
    "            dists = actor(t(self.state))\n",
    "            actions = dists.sample().detach().data.numpy()\n",
    "            actions_clipped = np.clip(actions, self.env.action_space.low.min(), env.action_space.high.max())\n",
    "\n",
    "            next_state, reward, self.done, info = self.env.step(actions_clipped)\n",
    "            memory.append((actions, reward, self.state, next_state, self.done))\n",
    "\n",
    "            self.state = next_state\n",
    "            self.steps += 1\n",
    "            self.episode_reward += reward\n",
    "            \n",
    "            if self.done:\n",
    "                self.episode_rewards.append(self.episode_reward)\n",
    "                if len(self.episode_rewards) % 10 == 0:\n",
    "                    print(\"episode:\", len(self.episode_rewards), \", episode reward:\", self.episode_reward)\n",
    "                writer.add_scalar(\"episode_reward\", self.episode_reward, global_step=self.steps)\n",
    "                    \n",
    "        \n",
    "        return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "writer = SummaryWriter(\"runs/mish_activation\")\n",
    "\n",
    "# config\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "actor = Actor(state_dim, n_actions, activation=Mish)\n",
    "critic = Critic(state_dim, activation=Mish)\n",
    "\n",
    "learner = A2CLearner(actor, critic)\n",
    "runner = Runner(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10 , episode reward: -1331.6603030893166\n",
      "episode: 20 , episode reward: -1308.5052810626362\n",
      "episode: 30 , episode reward: -1295.3439224521233\n",
      "episode: 40 , episode reward: -1264.9938388654693\n",
      "episode: 50 , episode reward: -1159.1376888101245\n",
      "episode: 60 , episode reward: -1096.290611383684\n",
      "episode: 70 , episode reward: -1332.3170406146635\n",
      "episode: 80 , episode reward: -529.9847037752162\n",
      "episode: 90 , episode reward: -823.6640709812223\n",
      "episode: 100 , episode reward: -716.3981973346959\n",
      "episode: 110 , episode reward: -267.55884160232574\n",
      "episode: 120 , episode reward: -1183.7605896140449\n",
      "episode: 130 , episode reward: -566.974616549244\n",
      "episode: 140 , episode reward: -137.6165269870926\n",
      "episode: 150 , episode reward: -691.5466228810094\n",
      "episode: 160 , episode reward: -662.1612165947158\n",
      "episode: 170 , episode reward: -1086.4806743221225\n",
      "episode: 180 , episode reward: -647.1489689713651\n",
      "episode: 190 , episode reward: -1238.4159859610431\n",
      "episode: 200 , episode reward: -263.25376664267634\n",
      "episode: 210 , episode reward: -271.50743676771833\n",
      "episode: 220 , episode reward: -130.07883036823304\n",
      "episode: 230 , episode reward: -982.4589363644424\n",
      "episode: 240 , episode reward: -1256.668380561799\n",
      "episode: 250 , episode reward: -1267.7317399215963\n",
      "episode: 260 , episode reward: -131.6000793348229\n",
      "episode: 270 , episode reward: -2.471643838360228\n",
      "episode: 280 , episode reward: -1374.7596483537313\n",
      "episode: 290 , episode reward: -132.09786114294778\n",
      "episode: 300 , episode reward: -283.41963077793525\n",
      "episode: 310 , episode reward: -287.6034814493751\n",
      "episode: 320 , episode reward: -126.69283889032661\n",
      "episode: 330 , episode reward: -1.133852311983069\n",
      "episode: 340 , episode reward: -266.44327146392425\n",
      "episode: 350 , episode reward: -273.25977412397543\n",
      "episode: 360 , episode reward: -3.144876597430591\n",
      "episode: 370 , episode reward: -1495.0052678631948\n",
      "episode: 380 , episode reward: -441.07285351652\n",
      "episode: 390 , episode reward: -132.61540054307525\n",
      "episode: 400 , episode reward: -137.02285450348222\n",
      "episode: 410 , episode reward: -133.8382833481163\n",
      "episode: 420 , episode reward: -1.4550722539388448\n",
      "episode: 430 , episode reward: -129.01782445352362\n",
      "episode: 440 , episode reward: -266.22297564145487\n",
      "episode: 450 , episode reward: -444.16119954649116\n",
      "episode: 460 , episode reward: -132.3216589765316\n",
      "episode: 470 , episode reward: -136.68120605004034\n",
      "episode: 480 , episode reward: -438.02075550748197\n",
      "episode: 490 , episode reward: -131.1251871496582\n",
      "episode: 500 , episode reward: -133.29030451790683\n"
     ]
    }
   ],
   "source": [
    "steps_on_memory = 16\n",
    "episodes = 500\n",
    "episode_length = 200\n",
    "total_steps = (episode_length*episodes)//steps_on_memory\n",
    "\n",
    "for i in range(total_steps):\n",
    "    memory = runner.run(steps_on_memory)\n",
    "    learner.learn(memory, runner.steps, discount_rewards=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
